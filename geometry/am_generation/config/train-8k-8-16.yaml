
# Global training setting
weight_dtype: fp16
output_dir: ./runs/
seed: 1337
weight_decay: 0.05
learning_rate: 5e-5
lr_scheduler_type: cosine_with_warmup
max_grad_norm: 1.0


# Checkpointing
load_pretrained: /aigc_cfs/weizhe/code/github/bpt/weights/bpt-8-16-500m.pt
conditioner_only: True
resume_from_checkpoint: null    # latest
checkpointing_steps: 500

# Tracking
with_tracking: true
report_to: wandb
config_file_name: wandb_config

# Schedule
num_warmup_steps: 4000
gradient_accumulation_steps: 1
num_train_epochs: 2000
max_train_steps: 200000000
eval_steps: 4000000000000000000000000000
max_eval_samples: 2


# Training Data, config should be the same as the model config
train_data:
    batch_size: 3  # Batchsize (per process) (Total training batchsize = nnodes * ngpus_per_node * batch_size)
    data_json:  /aigc_cfs/weizhe/code/git_moa/ArtistCreatedMeshes/jsons/subset_8K.json
    cluster: cfs # or "910b" or "lanjing"
    pkl_dir: /apdcephfs_cq8/share_1615605/weizheliu/bpt-8k-8-16-200k
    quantization_bits: 7
    augment: True
    pc_num: 4096
    block_size: 8
    offset_size: 16
    compressed: True
    special_token: -2
    use_special_block: True
    max_seq_len: 10000
# Validation Data
val_data:
    batch_size: 1  # Batchsize (per process) (Total training batchsize = nnodes * ngpus_per_node * batch_size)
    data_json: /aigc_cfs/weizhe/code/git_moa/ArtistCreatedMeshes/jsons/subset_8K.json
    cluster: cfs # or "910b" or "lanjing"
    pkl_dir: /apdcephfs_cq8/share_1615605/weizheliu/bpt-8k-8-16-200k
    quantization_bits: 7
    augment: True
    pc_num: 4096
    block_size: 8
    offset_size: 16
    compressed: True
    special_token: -2
    use_special_block: True
    max_seq_len: 10000

# Model config
model_config:
    dim: 1024  # hidden size of Transformer
    max_seq_len: 10000
    flash_attn: True
    attn_depth: 24
    attn_dim_head: 64
    attn_heads: 16                        # number of heads
    dropout: 0.0
    pad_id: -1
    num_discrete_coors: 128
    block_size: 8
    offset_size: 16
    mode: 'vertices'
    special_token: -2
    use_special_block: True
    conditioned_on_pc: True
    encoder_name: 'miche-256-feature'
    encoder_freeze: True
